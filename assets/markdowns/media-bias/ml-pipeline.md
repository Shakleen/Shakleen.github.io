## Problem
Processing and analyzing large-scale news datasets requires high-throughput compute and efficient memory management to keep costs low.

## Solution
I utilized a hybrid stack of Python (PyTorch/Hugging Face) for model training and Go for performance-critical backend processing.

## Result
Implemented a robust data ingestion and inference engine capable of handling high-volume text data through containerized workflows on Docker and Kubernetes.
